{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recurrent_neural_networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Different types of Recurrent Neural Networks (RNNs)"
      ],
      "metadata": {
        "id": "tDKPFw7BczJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y6cthgOmS0tb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Bidirectional"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "zxvIwmDBdCfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "## reshaping and normalizing mnist images  \n",
        "x_train = x_train.reshape([-1, 28, 28]).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape([-1, 28, 28]).astype(\"float32\") / 255.0"
      ],
      "metadata": {
        "id": "z4I-ldnGdB6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa923716-3bff-47f8-f17a-75f9241bec2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Simple Dense RNN \n",
        "\n",
        "RNN works on the principle of saving the output of a particular layer and feeding this \n",
        " back to the input in order to predict the output of the layer.<br>\n",
        "\n",
        " <center>\n",
        " <img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse3.explicit.bing.net%2Fth%3Fid%3DOIP.C-VkQoN0koBX4KxE5z-QYQHaC6%26pid%3DApi&f=1\" width=500 height=300>\n",
        " </center>\n",
        "\n",
        "<p>The decision a recurrent net reached at time step t-1 affects the decision it will reach one moment later at time step t.<br>\n",
        "\n",
        " So recurrent networks have two sources of input, the present and the recent past, which combine to determine how they respond to new data, much as we do in life.</p>"
      ],
      "metadata": {
        "id": "TItD-IBmcvkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(nmae=\"Simple_RNNs\")\n",
        "# our model takes 28 inputs and has 28 timesteps\n",
        "model.add(keras.Input(shape=(None, 28))) \n",
        "model.add(SimpleRNN(256, activation='tanh', return_sequences=True, name=\"hidden1\"))\n",
        "model.add(SimpleRNN(256, activation='tanh', return_sequences=False, name='hidden2'))\n",
        "model.add(keras.layers.Dense(10,  name='output_layer'))\n",
        "\n",
        "## Model Summary\n",
        "print(model.summary())\n",
        "\n",
        "## Compiling the model\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "\n",
        "## Training the model on MNIST dataset\n",
        "print(\"\\n\\nTraining The Model\")\n",
        "model.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "\n",
        "## Evaluating the model\n",
        "print(\"\\n\\nModel Evaluation\")\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgFfKwUwWu9a",
        "outputId": "21e1d3ec-9674-4b6d-ccb0-11744523a487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " hidden1 (SimpleRNN)         (None, None, 256)         72960     \n",
            "                                                                 \n",
            " hidden2 (SimpleRNN)         (None, 256)               131328    \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 206,858\n",
            "Trainable params: 206,858\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Training The Model\n",
            "Epoch 1/10\n",
            "938/938 - 98s - loss: 0.3080 - accuracy: 0.9067 - 98s/epoch - 104ms/step\n",
            "Epoch 2/10\n",
            "938/938 - 97s - loss: 0.1810 - accuracy: 0.9485 - 97s/epoch - 104ms/step\n",
            "Epoch 3/10\n",
            "938/938 - 96s - loss: 0.1571 - accuracy: 0.9552 - 96s/epoch - 102ms/step\n",
            "Epoch 4/10\n",
            "938/938 - 97s - loss: 0.1445 - accuracy: 0.9589 - 97s/epoch - 103ms/step\n",
            "Epoch 5/10\n",
            "938/938 - 101s - loss: 0.1438 - accuracy: 0.9590 - 101s/epoch - 107ms/step\n",
            "Epoch 6/10\n",
            "938/938 - 97s - loss: 0.1354 - accuracy: 0.9617 - 97s/epoch - 103ms/step\n",
            "Epoch 7/10\n",
            "938/938 - 99s - loss: 0.1380 - accuracy: 0.9606 - 99s/epoch - 105ms/step\n",
            "Epoch 8/10\n",
            "938/938 - 98s - loss: 0.1388 - accuracy: 0.9587 - 98s/epoch - 105ms/step\n",
            "Epoch 9/10\n",
            "938/938 - 96s - loss: 0.1323 - accuracy: 0.9618 - 96s/epoch - 102ms/step\n",
            "Epoch 10/10\n",
            "938/938 - 92s - loss: 0.1467 - accuracy: 0.9571 - 92s/epoch - 98ms/step\n",
            "\n",
            "\n",
            "Model Evaluation\n",
            "157/157 - 5s - loss: 0.1514 - accuracy: 0.9568 - 5s/epoch - 35ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1514495313167572, 0.9567999839782715]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTMs (Long Short-Term Meomory)\n",
        "<p>LSTMs help preserve the error that can be backpropagated through time and layers. By maintaining a more constant error, <br>they allow recurrent nets to continue to learn over many time steps (over 1000), thereby opening a channel to link causes and effects remotely. This is one of the central challenges to machine learning and AI, <br>since algorithms are frequently confronted by environments where reward signals are sparse and delayed, such as life itself.\n",
        "</p>\n",
        "<center>\n",
        "<h3>LSTM Cell</h3>\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.Wxt8Fv2FV3MOFvICx-GwUgHaFc%26pid%3DApi&f=1\">\n",
        "</center>\n",
        "\n",
        "### Creating a model using Bidirectional LSTM cells"
      ],
      "metadata": {
        "id": "CWEmYY3qV2DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = keras.Sequential(name=\"LSTMs\")\n",
        "\n",
        "model2.add(keras.Input(shape=(None, 28)))\n",
        "model2.add(LSTM(256, return_sequences=True, activation=\"relu\"))\n",
        "model2.add(keras.layers.Dropout(0.2))\n",
        "model2.add(LSTM(256, return_sequences=False))\n",
        "model2.add(keras.layers.Dropout(0.2))\n",
        "model2.add(keras.layers.Dense(10, name=\"output_layer\"))\n",
        "\n",
        "## Model Summary\n",
        "print(model2.summary())\n",
        "\n",
        "## Compiling the model\n",
        "model2.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "## Training the model on MNIST dataset\n",
        "print(\"\\n\\nTraining The Model\")\n",
        "model2.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "\n",
        "## Evaluating the model\n",
        "print(\"\\n\\nModel Evaluation\")\n",
        "model2.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "aKFeqzkIg-RT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0abeb95d-d07b-4ea1-b0b2-afe0ecdac252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"LSTMs\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_10 (LSTM)              (None, None, 256)         291840    \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, None, 256)         0         \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " output_layer (Dense)        (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 819,722\n",
            "Trainable params: 819,722\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Training The Model\n",
            "Epoch 1/10\n",
            "938/938 - 272s - loss: 0.3020 - accuracy: 0.9007 - 272s/epoch - 290ms/step\n",
            "Epoch 2/10\n",
            "938/938 - 268s - loss: 0.0873 - accuracy: 0.9734 - 268s/epoch - 286ms/step\n",
            "Epoch 3/10\n",
            "938/938 - 270s - loss: 0.0636 - accuracy: 0.9807 - 270s/epoch - 288ms/step\n",
            "Epoch 4/10\n",
            "938/938 - 270s - loss: 0.0512 - accuracy: 0.9847 - 270s/epoch - 288ms/step\n",
            "Epoch 5/10\n",
            "938/938 - 269s - loss: 0.0414 - accuracy: 0.9876 - 269s/epoch - 287ms/step\n",
            "Epoch 6/10\n",
            "938/938 - 271s - loss: 0.0348 - accuracy: 0.9900 - 271s/epoch - 288ms/step\n",
            "Epoch 7/10\n",
            "938/938 - 271s - loss: 0.0324 - accuracy: 0.9904 - 271s/epoch - 289ms/step\n",
            "Epoch 8/10\n",
            "938/938 - 272s - loss: 0.0271 - accuracy: 0.9917 - 272s/epoch - 290ms/step\n",
            "Epoch 9/10\n",
            "938/938 - 269s - loss: 0.0249 - accuracy: 0.9926 - 269s/epoch - 287ms/step\n",
            "Epoch 10/10\n",
            "938/938 - 295s - loss: 0.0231 - accuracy: 0.9925 - 295s/epoch - 314ms/step\n",
            "\n",
            "\n",
            "Model Evaluation\n",
            "157/157 - 17s - loss: 0.0385 - accuracy: 0.9880 - 17s/epoch - 106ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03847460821270943, 0.9879999756813049]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRUs (Gated Recurrent Units)\n",
        "\n",
        "<p>The problem that arose when LSTM’s where initially introduced was the high number of parameters. <br>Let’s start by saying that the motivation for the proposed LSTM variation called GRU is the simplification, in terms of the number of parameters and the performed operations.</p>\n",
        "<p>GRU’s got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate.</p>\n",
        "\n",
        "<center>\n",
        "<h3>GRU Cell</h3>\n",
        "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.DTOtKM1e1LLlo7VgBgH8EwHaDg%26pid%3DApi&f=1\" width=500 height=300>\n",
        "</center>\n",
        "\n",
        "### Creating model using GRU cells"
      ],
      "metadata": {
        "id": "_1hTFtD4vaXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = keras.Sequential(name=\"GRUs\")\n",
        "\n",
        "model3.add(keras.Input(shape=(None, 28)))\n",
        "model3.add(GRU(256, return_sequences=True, activation='relu'))\n",
        "model3.add(keras.layers.Dropout(0.3))\n",
        "model3.add(GRU(128, return_sequences=False))\n",
        "model3.add(keras.layers.Dense(10))\n",
        "\n",
        "## Model Summary\n",
        "print(model3.summary())\n",
        "\n",
        "## Compiling the model\n",
        "model3.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3, decay=1e-5),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "## Training the model on MNIST dataset\n",
        "print(\"\\n\\nTraining The Model\")\n",
        "model3.fit(x_train, y_train, batch_size=64, epochs=10, verbose=2)\n",
        "\n",
        "## Evaluating the model\n",
        "print(\"\\n\\nModel Evaluation\")\n",
        "model3.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "id": "pVD10rqvoY1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82672f78-e597-4b11-b0ef-be5eb60c7d25"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"GRUs\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru (GRU)                   (None, None, 256)         219648    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 256)         0         \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 128)               148224    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 369,162\n",
            "Trainable params: 369,162\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "\n",
            "\n",
            "Training The Model\n",
            "Epoch 1/10\n",
            "938/938 - 168s - loss: 0.3092 - accuracy: 0.8959 - 168s/epoch - 179ms/step\n",
            "Epoch 2/10\n",
            "938/938 - 160s - loss: 0.0759 - accuracy: 0.9771 - 160s/epoch - 171ms/step\n",
            "Epoch 3/10\n",
            "938/938 - 160s - loss: 0.0542 - accuracy: 0.9834 - 160s/epoch - 170ms/step\n",
            "Epoch 4/10\n",
            "938/938 - 159s - loss: 0.0437 - accuracy: 0.9861 - 159s/epoch - 170ms/step\n",
            "Epoch 5/10\n",
            "938/938 - 159s - loss: 0.0358 - accuracy: 0.9887 - 159s/epoch - 169ms/step\n",
            "Epoch 6/10\n",
            "938/938 - 160s - loss: 0.0298 - accuracy: 0.9906 - 160s/epoch - 171ms/step\n",
            "Epoch 7/10\n",
            "938/938 - 158s - loss: 0.0243 - accuracy: 0.9922 - 158s/epoch - 168ms/step\n",
            "Epoch 8/10\n",
            "938/938 - 157s - loss: 0.0220 - accuracy: 0.9932 - 157s/epoch - 168ms/step\n",
            "Epoch 9/10\n",
            "938/938 - 160s - loss: 0.0202 - accuracy: 0.9935 - 160s/epoch - 170ms/step\n",
            "Epoch 10/10\n",
            "938/938 - 158s - loss: 0.0172 - accuracy: 0.9940 - 158s/epoch - 169ms/step\n",
            "\n",
            "\n",
            "Model Evaluation\n",
            "157/157 - 9s - loss: 0.0385 - accuracy: 0.9900 - 9s/epoch - 55ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03848262503743172, 0.9900000095367432]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problems with RNNs\n",
        "\n",
        "<p> As we can see, LSTMs and GRUs works better than simple RNNs. As we can see GRUs can perform as better as LSTMs with less number of parameters. <br> Here, GRU performs better for this dataset but we can't say that it will be a better choice everytime over LSTMs.<br>\n",
        "RNN and LSTM are difficult to train because they require memory-bandwidth-bound computation, which is the worst nightmare for hardware designers.<br>\n",
        "LSTM require 4 linear layer (MLP layer) per cell to run at and for each sequence time-step. Linear layers require large amounts of memory bandwidth to be computed, in fact they cannot use many compute unit often because the system has not enough memory bandwidth to feed the computational units.<br>\n",
        "A comparison of the effectiveness of LSTM and Transformer (attention based model) shows that attention is usually attention wins.\n",
        "</p>"
      ],
      "metadata": {
        "id": "5XMBmcVS4-2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6cro8rzT--h8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}